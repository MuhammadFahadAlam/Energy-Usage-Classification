---
title: "sasasas"
output: html_document
---

## Packages 

```{r}
#### Loading Packages ####

	library(tidyverse)
  library(cluster)
  library(class)
  library(dendextend)
  library(factoextra)
  library(dbscan)
  library(fpc)
  library(FactoMineR)
  library(ggfortify)
  library(wordcloud)
  library(RColorBrewer)
  library(tm)
  library(SnowballC)
  library(reshape2)
  library(corrplot)
  library(caret)
  library(caTools)
  library(pROC)
 
  
```




## Functions {#S1}

```{r}

#### sums NA values then uses the apply family of functions to obtain NA values per column ####

## Get the sum of NA elements in columns of dataframe. Used for checking if all the NA are removed.

AppSumNa = function(x){
  ty = is.na(x)
  yt= function(y){
    sum(ty)
  } 
  yy = apply(x, 2, yt)
  return(yy)
}

#### MinMax normalization ####

## To normalize our data in range 0 - 1. It helps to make all columns comparable and improve accuracy.

MinMax=function(x){
  ty = (x-min(x))/(max(x)-min(x))
  
}

#### Z-score normalisation ####

## Get the z-values

Zscore = function(x){

    mu = mean(x)
    sd = sd(x)
    op = ((x - mu) / sd )
  
}

#### Stratified Sampling Function ####

## USed for sampling population in a way that each group has equal representation in the sample. 
## This helps in controlling overfitting problem and generalize on all possible classes.

stratified <- function(df, group, size, select = NULL, 
                       replace = FALSE, bothSets = FALSE) {
  if (is.null(select)) {
    df <- df
  } else {
    if (is.null(names(select))) stop("'select' must be a named list")
    if (!all(names(select) %in% names(df)))
      stop("Please verify your 'select' argument")
    temp <- sapply(names(select),
                   function(x) df[[x]] %in% select[[x]])
    df <- df[rowSums(temp) == length(select), ]
  }
  df.interaction <- interaction(df[group], drop = TRUE)
  df.table <- table(df.interaction)
  df.split <- split(df, df.interaction)
  if (length(size) > 1) {
    if (length(size) != length(df.split))
      stop("Number of groups is ", length(df.split),
           " but number of sizes supplied is ", length(size))
    if (is.null(names(size))) {
      n <- setNames(size, names(df.split))
      message(sQuote("size"), " vector entered as:\n\nsize = structure(c(",
              paste(n, collapse = ", "), "),\n.Names = c(",
              paste(shQuote(names(n)), collapse = ", "), ")) \n\n")
    } else {
      ifelse(all(names(size) %in% names(df.split)),
             n <- size[names(df.split)],
             stop("Named vector provided with names ",
                  paste(names(size), collapse = ", "),
                  "\n but the names for the group levels are ",
                  paste(names(df.split), collapse = ", ")))
    }
  } else if (size < 1) {
    n <- round(df.table * size, digits = 0)
  } else if (size >= 1) {
    if (all(df.table >= size) || isTRUE(replace)) {
      n <- setNames(rep(size, length.out = length(df.split)),
                    names(df.split))
    } else {
      message(
        "Some groupings\n---",
        paste(names(df.table[df.table < size]), collapse = ", "),
        "---\ncontain less observations",
        " than required number of samples.\n",
        "All observations have been returned from their groups.")
      n <- c(sapply(df.table[df.table >= size], function(x) x = size),
             df.table[df.table < size])
    }
  }
  temp <- lapply(
    names(df.split),
    function(x) df.split[[x]][sample(df.table[x],
                                     n[x], replace = replace), ])
  set1 <- do.call("rbind", temp)
  
  if (isTRUE(bothSets)) {
    set2 <- df[!rownames(df) %in% rownames(set1), ]
    list(SET1 = set1, SET2 = set2)
  } else {
    set1
  }
}

```



## Importing and Inspecting datasets {#S2}

```{r Importing datasets}

#### Loading information household dataset ####

inform = read.csv("OneDrive_1_4-14-2021/informations_households.csv")

#### Loading daily consumption dataset ####

daily = read.csv("OneDrive_1_4-14-2021/dd.csv")

#### Loading daily weather dataset ####

weather = read.csv("OneDrive_1_4-14-2021/weather_daily_darksky.csv")

```

## Data Preparation before joining {#S3}

```{r Selecting important columns from weather dataset}

#### Removing columns that are not required from the weather dataset before joining ####

## Replacing loaded Dataframe with a new dataframe containing on the indicated index columns. All other columns not in the c( ... ) are removed.

weather = weather[,c(1,4,7,8,12,13,14,19,22,23,27)]

#### Changing weather to date format for the join between datasets ####

## Weather$time => time column in weather dataframe
## time column is a Date string type field. We need to convert it into date object format to make it useful for our analysis and visualization.
## as.Date(Object, format = "Format of the Object")

weather$time = as.Date(weather$time)

```



## Joining 3 Datasets {#S4}

```{r Joining three relevant datasets}

#### Joining daily dataset using local id column ####

## Joining daily and inform dataframes(loaded previously) on the common key 'LCLid'. It is an inner join.

joined_set = inner_join(daily,inform,by="LCLid")

## day column of joined_set is an string We are converting it into object type.

joined_set$day = as.Date(joined_set$day,"%d/%m/%Y")
joined_set$day =  as.Date(joined_set$day)

# Joining weather data to the dataset

## Joining Joined_set and weather ob Day and Time Columns of respective dataframes.

london_smart_meter = inner_join(joined_set,weather,by=c("day"="time"))

```



## Creating new features and renaming columns for analysis {#S5}

```{r Creating new features}

# Creating a variable to store date

## storing date of joined dataframe of all 3 datas.

date = london_smart_meter$day

# find day and month by using Date object

date_day = weekdays(date)
date_month = months(date)

# Transform date into character

## convert date object to string

date = as.character(date)

# Subset string to extract desired characters

## saving last 4 string characters in variable

character_manipulation = stringr::str_sub(date,end = 4)

## last 4 values in the string are year.
london_smart_meter$year = character_manipulation
london_smart_meter$weekday = date_day
london_smart_meter$month = date_month

# Removing a ACORN data quality issue

## %>% is used to pass left side as an argument of function at right side

london_smart_meter = london_smart_meter %>% 
  filter(Acorn_grouped !="ACORN-",
         Acorn_grouped !="ACORN-U",
         year!="2011")

# Renaming day column as date
london_smart_meter = london_smart_meter %>% 
  rename(date = day)
  

```



## Stratified Sampling {#S6}

```{r}

# Set seed for reproducible results

set.seed(1)

# Preserving original set temporarily for comparison

comparison = london_smart_meter

# Stratified sampling arguments

london_smart_meter = stratified(london_smart_meter, "Acorn_grouped", 0.02)

# Checking to make sure that sample is stratified according to Acorn_grouped

london_smart_meter %>% 
    count(Acorn_grouped) %>% 
    mutate(total=nrow(london_smart_meter)) %>% 
    summarise(Acorn_grouped,n, n/total)

comparison %>% 
    count(Acorn_grouped) %>% 
    mutate(total=nrow(comparison)) %>% 
    summarise(Acorn_grouped,n, n/total)

# Removing the variables used for verification of the sampling preserving the data's structural properties  

rm(comparison)

# Removing unnecessary variables to avoid clogging up ram

rm(inform)
rm(weather)
rm(joined_set)
rm(date)
rm(date_day)
rm(date_month)
rm(daily)

```


## Data Cleaning and preparation part- I {#S7}

```{r Data Cleaning and Preparation-Part-I}

# Checking the sum of NA's in the data
 AppSumNa(london_smart_meter)

# Dropping NA's in question

noNa_london_smart_meter= na.omit(london_smart_meter)
rm(london_smart_meter)

#Re-ordering the columns in the data

smart_meter_london = noNa_london_smart_meter[,c(1,2,24,25,26,10,11,12,3,6,4,5,7,
                                                8,9,14,16,17,19,20,21,22,23,18,15,13)]

# Checking if there are any remaining NA's 

AppSumNa(smart_meter_london)

# Removing no NA dataset

rm(noNa_london_smart_meter)

```




## Partial Data Cleaning and Preparation- II {#S8}

```{r Changing variable types}
#Data cleaning and preparation.

## Encoding categorical data to numerical data to process.

smart_meter_london$icon = as.factor(smart_meter_london$icon)
smart_meter_london$stdorToU = as.factor(smart_meter_london$stdorToU)
smart_meter_london$Acorn = as.character(smart_meter_london$Acorn)
smart_meter_london$Acorn_grouped = as.factor(smart_meter_london$Acorn_grouped)
smart_meter_london$precipType = as.factor(smart_meter_london$precipType)
smart_meter_london$Acorn = as.factor(smart_meter_london$Acorn)
smart_meter_london$year = as.factor(smart_meter_london$year)
smart_meter_london$weekday = as.factor(smart_meter_london$weekday)
smart_meter_london$month = as.factor(smart_meter_london$month)

# Checking the structure to make sure the changes were succesful
str(smart_meter_london)

```



# Data Quality check {#S9}

```{r}
library(validate)
# Validating the quality of the data

smart_meter_rules = validate::validator(NonNegMedian = energy_median >= 0,
                              NonNegMean = energy_mean >= 0,
                              NonNegMax = energy_max >= 0,
                              NonNegCount = energy_count >= 0,
                              EnergyCountVal = energy_count == 48,
                              NonNegStd = energy_std >= 0,
                              NonNegSum = energy_sum >= 0 & energy_sum < 23,
                              NonNegMin = energy_min >= 0,
                              okMaxTemp = temperatureMax <= 38.7,
                              okMinTemp = temperatureMin >= -27,
                              NonNegwindBearing = windBearing >=0,
                              NonNegcloudCover = cloudCover >= 0,
                              NonNegwindSpeed = windSpeed >= 0,
                              NonNegvisibility = visibility >=0,
                              NonNeghumidity = humidity >= 0,
                              okcloudcover = cloudCover >= 0)

 
## confront is used to findout

check <- confront(smart_meter_london, smart_meter_rules)  

 


#check

 

# jpeg('validation.jpg')
plot(check)

```

```{R}
# Cleaning the quality issue with energy count

## only saving energy count = 48 value in var.

smart_meter_london = smart_meter_london %>% 
  filter(energy_count == 48)
  


```

Data quality issue.

## Creating Categorical Variable from energy

```{r}
# Variable creation

# Creating labels for the classifier

## Enery Usage is Output. it is numerical value so we are converting it to categorical data to apply classification on it.

# <4.5 => low, >5 & <7.0 => medium, >7.0 => high

smart_meter_london$energy_usage = ifelse(smart_meter_london$energy_sum <= 4.5, 'low', ifelse(smart_meter_london$energy_sum > 5 | smart_meter_london$energy_sum >= 7, 'medium', 'high'))

# Expressing energy_usage as an ordinal variables with levels from low to high. In ordinal data the level increases .

smart_meter_london$energy_usage = factor(smart_meter_london$energy_usage,order = TRUE, levels=c("low", "medium", "high"))


```



# Exploratory Data Analysis- Smart Meter London {#S10}

## Word Cloud part - I {#S16}   

```{r Word Cloud Prep,eval=FALSE}

# setting seed for the reproducibility of the word cloud

set.seed(8)

# Collapsing spaces and storing the values inside a variable without spaces

words = paste(smart_meter_london$summary, collapse = " ")

# Removing superfluous words

words = stringr::str_replace_all(words,"throughout","")
words = stringr::str_replace_all(words,"mostly","")
words = stringr::str_replace_all(words,"partly","")

# Wordcloud generation

wordcloud(words = words, min.freq = 1,
          max.words=1000, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

# Removing unnecessary components that will not be used in the EDA

smart_meter_london$summary = NULL
rm(character_manipulation)

```


## Descriptive Statistics

```{r}

summary(smart_meter_london)

sum(smart_meter_london$energy_sum)

# Aggregation

## Total number of observation

n = nrow(smart_meter_london)

## group by output and fid out the solution.

b = smart_meter_london %>% 
  group_by(Acorn_grouped) %>% 
  summarise(total_energy = sum(energy_sum)) %>% 
  ggplot(aes(x=total_energy)) + geom_bar()


smart_meter_london %>% 
  group_by(Acorn_grouped) %>% 
  summarise(total_energy = sum(energy_sum)) 

smart_meter_london %>% 
  group_by(precipType) %>% 
  summarise(total_energy = sum(energy_sum)) 

smart_meter_london %>% 
  group_by(icon) %>% 
  summarise(total_energy = sum(energy_sum)) 

smart_meter_london %>% 
  group_by(icon) %>% 
  summarise(total_energy = sum(energy_sum)) 


# Exploratory Data Analysis

## making histograms

par(mfrow=c(3,2))
hist(smart_meter_london$temperatureMax)
hist(smart_meter_london$windSpeed)
hist(smart_meter_london$energy_max)
hist(smart_meter_london$energy_mean,xlim = c(0,3))
hist(smart_meter_london$temperatureMin)

#

smart_meter_london %>%
ggplot(aes(x=date,y=energy_sum)) + geom_line(linetype="solid",alpha=0.25,colour="dodgerblue3")  


smart_meter_london %>% 
  group_by(icon) %>% 
  summarise(total_energy = sum(energy_sum),icon) %>% 
  ggplot(aes(x=icon)) + geom_bar() 
  
# Determine how many precipTypes there are before transforming to numeric 
smart_meter_london %>% 
  group_by(precipType,energy_sum) %>% 
  count(precipType) %>% 
  ggplot(aes(x=precipType)) + geom_bar() 

p = smart_meter_london %>% 
  select(energy_sum,month) %>% 
  ggplot(aes(x=month)) + geom_bar()

p 

m = smart_meter_london %>% 
  select(energy_sum,weekday) %>% 
  ggplot(aes(x=weekday)) + geom_bar() 

m

# Exploring how man users are using STD or ToU

smart_meter_london %>% 
  select(energy_sum,stdorToU) %>% 
  ggplot(aes(x=stdorToU)) + geom_bar() 

smart_meter_london %>% 
    group_by(Acorn,Acorn_grouped,energy_sum) %>% 
    summarise(energy_total = sum(energy_sum)) %>% 
    count(Acorn,Acorn_grouped,energy_sum) %>% 
    ggplot(aes(x=Acorn_grouped,colour=factor(Acorn))) + 
    geom_bar(stat = "count",position ="stack",fill="white") +
    theme_classic()

smart_meter_london %>% 
    group_by(Acorn,Acorn_grouped,energy_sum) %>% 
    summarise(energy_total = sum(energy_sum)) %>% 
    count(Acorn,Acorn_grouped) %>% 
    ggplot(aes(x=Acorn_grouped,colour=factor(Acorn))) + 
    geom_bar(stat = "count",position ="stack",fill="white") +
    theme_classic()

```


## Correlation map


```{r Correlations}
# Create Correlation map of variables

corr.set <- select_if(smart_meter_london,is.numeric)
corr.matrix = round(cor(corr.set),2)
corr.matrix 

# Correlation Heat Map - Upper Triangle

get_upper_tri <- function(corr.matrix){
    corr.matrix[lower.tri(corr.matrix)]= NA
    return(corr.matrix)
}

# using upper triangle function to return the upper half of the correlation map

upper_tri <- get_upper_tri(corr.matrix)
upper_tri

# Correlation Heat Map - Upper Triangle

melt_cormat <- melt(upper_tri, na.rm = TRUE)

# Heatmap

ggheatmap = ggplot(data = melt_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()

# Adding correlation coefficients on the heatmap

corelcoef = ggheatmap + geom_text(aes(Var2, Var1, label = value), color = "black", size = 2) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))

# Print the correlation coefficient 

corelcoef

# Removing numeric set

rm(numericset)

```


## Variable Selection


```{r Feature Selection}

numeric.set = smart_meter_london %>% 
  select(energy_sum, temperatureMax, cloudCover, 
         windSpeed, visibility, humidity, temperatureMin)

# add energy_mean

```



## Principal Component Analysis    

```{r} 

# Get principle components

pca.smart.meter = PCA(numeric.set,scale.unit = T,ncp = 5,graph = T)
# pca.smart.meter = PCA(numeric.set[,3:8],scale.unit = T,ncp = 5,graph = T)

# get eigen values 

get_eigenvalue(pca.smart.meter)

# Visualizing eigen values 

fviz_eig(pca.smart.meter, addlabels = TRUE, ylim = c(0, 50))

# 

fviz_pca_var(pca.smart.meter, col.var = "black")


#

var <- get_pca_var(pca.smart.meter)
var


#
head(var$coord, 4)

# Plotting variable contribution 

corrplot(var$cos2, is.corr=FALSE)

# Variable contribution to two major principle components

# fviz_contrib(pca.smart.meter, choice = "var", axes = 1:3, top = 10)

fviz_contrib(pca.smart.meter, choice = "var", axes = 1, top = 10)

fviz_contrib(pca.smart.meter, choice = "var", axes = 2, top = 10)

fviz_contrib(pca.smart.meter, choice = "var", axes = 3, top = 10)


# Plotting variable contribution

fviz_pca_var(pca.smart.meter, col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")
)

#

set.seed(123)
res.km <- kmeans(var$coord, centers = 3, nstart = 25)
grp <- as.factor(res.km$cluster)


# Color variables by groups

fviz_pca_var(pca.smart.meter, col.var = grp,
palette = c("#0073C2FF", "#EFC000FF", "#868686FF"),
legend.title = "Cluster")


res.desc <- dimdesc(pca.smart.meter, axes = c(1,2), proba = 0.05)

res.desc$Dim.1
res.desc$Dim.2


```

## MFA

```{r}

# removing variables that are unnecessary for MFA and the rest of the analysis.

smart_meter_london$file = NULL
smart_meter_london$energy_mean = NULL
smart_meter_london$energy_max = NULL
smart_meter_london$energy_std = NULL
smart_meter_london$energy_median = NULL
smart_meter_london$energy_count = NULL
smart_meter_london$energy_min = NULL
smart_meter_london$temperatureHigh = NULL
smart_meter_london$summary = NULL


#  selecting variables that will form the multiple factor analysis set.

mfa_set = smart_meter_london[,3:18]

mfa_set$Acorn = NULL


mfa_set= mfa_set[,c(1,2,3,4,5,13,14,15,6,7,8,9,10,11,12)]


#

mfa_set$year = NULL

# Converting factors to numerical representations before passing them to the MfA

mfa_set$Acorn_grouped = as.numeric(mfa_set$Acorn_grouped,"Adversity",1,"Affluent",2,"Comfortable",3)
mfa_set$stdorToU = as.numeric(mfa_set$stdorToU,"Std",1,"ToU",2)
mfa_set$precipType = as.numeric(mfa_set$precipType,"rain",1,"snow",2)
mfa_set$icon = as.numeric(mfa_set$icon,"clear-day",1,"cloudy",2,"fog",3,
                                      "partly-cloudy-day",4,"partly-cloudy-night",5,"wind",6)
mfa_set$weekday = as.numeric(mfa_set$weekday,"Monday",1,"Tuesday",2,"Wednesday",3,"Thursday",4,"Friday",5,"Saturday",6,"Sunday",7)
mfa_set$month = as.numeric(mfa_set$weekday,"January",1,"Febuary",2,"March",3,"April",4,"May",5,"June",6,"July",7,"August",8,"September",9,"October",10,"November",11,"December")
mfa_set$energy_usage = as.numeric(mfa_set$energy_usage,"low",1,"medium",2,"high",3)

# Applying MFA formula to dataset

res.mfa <- MFA(mfa_set,
group = c(7,7),
type = c("c","s"),
name.group = c("categorical","numerical"),
num.group.sup = NULL,
graph = FALSE)


# Extract Eignvalues and their cummulative sum

get_eigenvalue(res.mfa)

# Scree plot

fviz_screeplot(res.mfa)

# Plotting results of MFA according to their groups

fviz_mfa_var(res.mfa, "group")

# Visualizing multiple factor through variable contribution to pc1 

fviz_contrib(res.mfa, "group", axes = 1)

# Visualizing multiple factor through variable contribution to pc2

fviz_contrib(res.mfa, "group", axes = 2)

# Showing contribution seperated by categorical and numerical variables

fviz_mfa_var(res.mfa, "quanti.var", palette = "jco",
col.var.sup = "violet", repel = TRUE)


# Plotting contribution of variables using barchart representation

fviz_contrib(res.mfa, choice = "quanti.var", axes = 1, top = 20,
palette = "jco")

# Plotting contribution of variables using biplot representation 

fviz_mfa_var(res.mfa, "quanti.var", col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
col.var.sup = "violet", repel = TRUE,
geom = c("point", "text"))

# Remove mfa_set for memory manangement

rm(mfa_set)

```


## Energy Consumption - Bi-variate plots

```{r}

# Plotting consumption according 

#Observe Energy correlation values against other features
cor(numeric.set,smart_meter_london$energy_sum)


# Expressing Monthly data as seasonal quarters
## Get the months of observations
smart_meter_london$month <- factor(format(smart_meter_london$date, format = "%b"), levels = month.abb)
## Format for the seasonal quarters
smart_meter_london$quarter <- character(length = NROW(smart_meter_london))

 

smart_meter_london$quarter[smart_meter_london$month %in% month.abb[c(12,1,2)]] <- "Winter"
smart_meter_london$quarter[smart_meter_london$month %in% month.abb[c(3:5)]] <- "Spring"
smart_meter_london$quarter[smart_meter_london$month %in% month.abb[c(6:8)]] <- "Summer"
smart_meter_london$quarter[smart_meter_london$month %in% month.abb[c(9:11)]] <- "Autumn"
smart_meter_london$quarter <- factor(smart_meter_london$quarter, order = TRUE, levels = c("Spring","Summer","Autumn","Winter"))

 


# Plotting Energy consumption against other features 
#Energy v/s Max temp
ggplot(data = smart_meter_london, aes(x = temperatureMax, y = energy_usage)) + geom_boxplot(aes(color = quarter)) +
scale_fill_manual(values = c("#00AFBB", "#E7B800", "#FC4E07","#CC0000", "#006600", "#669999", "#00CCCC", "#660099", "#CC0066", "#FF9999", "#FF9900", "black"))+ facet_wrap(~quarter)+ labs(title = "Energy consumption v/s Max temp") 

 

#Energy v/s Min temp
ggplot(data = smart_meter_london, aes(x = temperatureMin, y = energy_usage)) + geom_boxplot(aes(color = quarter)) +
scale_fill_manual(values = c("#00AFBB", "#E7B800", "#FC4E07","#CC0000", "#006600", "#669999", "#00CCCC", "#660099", "#CC0066", "#FF9999", "#FF9900", "black"))+ facet_wrap(~quarter)+ labs(title = "Energy consumption v/s Min Temp")

 

#Energy v/s Windspeed
ggplot(data = smart_meter_london, aes(x = windSpeed, y = energy_usage)) + geom_boxplot(aes(color = quarter)) +
scale_fill_manual(values = c("#00AFBB", "#E7B800", "#FC4E07","#CC0000", "#006600", "#669999", "#00CCCC", "#660099", "#CC0066", "#FF9999", "#FF9900", "black"))+ facet_wrap(~quarter)+ labs(title = "Energy consumption v/s Windspeed")

 

#Energy v/s visibility
ggplot(data = smart_meter_london, aes(x = visibility, y = energy_usage)) + geom_boxplot(aes(color = quarter)) +
scale_fill_manual(values = c("#00AFBB", "#E7B800", "#FC4E07","#CC0000", "#006600", "#669999", "#00CCCC", "#660099", "#CC0066", "#FF9999", "#FF9900", "black"))+ facet_wrap(~quarter)+ labs(title = "Energy consumption v/s visibility")

 

#Energy consumption v/s cloudCover
ggplot(data = smart_meter_london, aes(x = cloudCover, y = energy_usage)) + geom_boxplot(aes(color = quarter)) +
scale_fill_manual(values = c("#00AFBB", "#E7B800", "#FC4E07","#CC0000", "#006600", "#669999", "#00CCCC", "#660099","#CC0066", "#FF9999", "#FF9900", "black"))+ facet_wrap(~quarter)+ labs(title = "Energy consumption v/s cloudCover")

 

#Energy consumption v/s humidity
ggplot(data = smart_meter_london, aes(x = humidity, y = energy_usage)) + geom_boxplot(aes(color = quarter)) +
scale_fill_manual(values = c("#00AFBB", "#E7B800", "#FC4E07","#CC0000", "#006600", "#669999", "#00CCCC", "#660099","#CC0066", "#FF9999", "#FF9900", "black"))+ facet_wrap(~quarter)+ labs(title = "Energy consumption v/s humidity")

 

#Energy consumption v/s Icon
ggplot(data = smart_meter_london, aes(x = icon, y = energy_sum)) + geom_boxplot(fill = "red") + coord_cartesian(ylim = c(0, 150))+  labs(title = "icon v/s consumption")

#Energy consumption v/s PrecipType
ggplot(data = smart_meter_london, aes(x = precipType, y = energy_sum)) + geom_boxplot(fill = "green")+
  labs(title = "precipType v/s consumption")


# Clearing memory 

rm(numeric.set)
smart_meter_london$quarter = NULL
rm(res.mfa)
rm(pca.smart.meter)

```



## Removing outliers

```{r}

# Interquartile range using summary function

summary(smart_meter_london$energy_sum)

#  Boxplot displaying outliers 

smartmeterboxplot = boxplot(smart_meter_london$energy_sum,horizontal = TRUE,col = "lightblue", main="Daily Energy Consumption",xlab= "Kilowatts per day")

# Dispensing of outliers 

min_energysum = min(smartmeterboxplot$out)

# Storing the outlier free dataset

smart_meter_london = smart_meter_london[smart_meter_london$energy_sum < min_energysum , ]

# Checking to ensure dataset does not contain outliers

summary(smart_meter_london$energy_sum)
boxplot(smart_meter_london$energy_sum,horizontal = TRUE,col = "lightblue", main="Daily Energy Consumption",xlab= "Kilowatts per day")


```




# Pre-processing data for Machine Learning 

```{r Data Preparation for Machine Learning}

# Managing the numeric 

ml.set = select_if(smart_meter_london,is.numeric)
# ml.set = ml.set[,c(9:18)]

# Apply Scaling

ml.set = apply(ml.set, 2, MinMax)
ml.set = as.data.frame(ml.set)
# ml.set = apply(ml.set, 2, Zscore) # If you need to use ZSCORE

# Converting numeric 

fac = smart_meter_london[,c(4:8,17:18)]
fac$Acorn_grouped = as.numeric(fac$Acorn_grouped,"Adversity",1,"Affluent",2,"Comfortable",3)
fac$stdorToU = as.numeric(fac$stdorToU,"Std",1,"ToU",2)
fac$icon = as.numeric(fac$icon,"clear-day",1,"cloudy",2,"fog",3,
                                      "partly-cloudy-day",4,"partly-cloudy-night",5,"wind",6)
fac$weekday = as.numeric(fac$weekday,"Monday",1,"Tuesday",2,"Wednesday",3,"Thursday",4,"Friday",5,"Saturday",6,"Sunday",7)
fac$month = as.numeric(fac$weekday,"January",1,"Febuary",2,"March",3,"April",4,"May",5,"June",6,"July",7,"August",8,"September",9,"October",10,"November",11,"December")
fac$energy_usage = as.numeric(fac$energy_usage,"low",1,"medium",2,"high",3)

#
fac$Acorn = NULL
fac$cloudcover = NULL
#
acorn = smart_meter_london$Acorn

#
ml.set$energy_mean  = NULL
#
ml_set = cbind(ml.set,fac)


#
rm(fac)
rm(smart_meter_london)
rm(character_manipulation)
rm(pca.smart.meter)
rm(res.mfa)
rm(numeric.set)
rm(numericset)
rm(ml.set)


```

##converting target variable into factor
```{r}
ml_set$energy_usage <- factor(ml_set$energy_usage)
str(ml_set)

```

##data partition
```{r}
set.seed(123)
training.samples <- ml_set$energy_usage %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- ml_set[training.samples, ]
test.data <- ml_set[-training.samples, ]

print(dim(train.data)); print(dim(test.data))
```

##building support vector machines

#run algorithms using 10-fold cross validation
```{r}
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
```

#building svm
```{r}
set.seed(100)
fit.svm <- train(energy_usage~., data = train.data, method = "svmRadial", metric = metric, trControl = control)

print(fit.svm)
```
##predicting test data


```{r}
predictions <- predict(fit.svm, test.data)
confusionMatrix(predictions, test.data$energy_usage)

```






